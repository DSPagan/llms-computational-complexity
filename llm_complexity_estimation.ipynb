{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP87xfgYGf4gvVhaWkyNlur",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DSPagan/llms-computational-complexity/blob/main/llm_complexity_estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the datasets"
      ],
      "metadata": {
        "id": "eme_SWVPo9SU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7DR5TChoHO7"
      },
      "outputs": [],
      "source": [
        "user = \"DSPagan\"\n",
        "repo = \"llms-computational-complexity\"\n",
        "src_dir = \"data\"\n",
        "train_file = \"train_data.jsonl\"\n",
        "test_file = \"test_data.jsonl\"\n",
        "\n",
        "url_1 = f\"https://raw.githubusercontent.com/{user}/{repo}/main/{src_dir}/{train_file}\"\n",
        "url_2 = f\"https://raw.githubusercontent.com/{user}/{repo}/main/{src_dir}/{test_file}\"\n",
        "\n",
        "!wget --no-cache --backups=1 {url_1}\n",
        "!wget --no-cache --backups=1 {url_2}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies"
      ],
      "metadata": {
        "id": "DwSnSCD24hma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo unsloth\n",
        "!pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer"
      ],
      "metadata": {
        "id": "3_GXQmh84hIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "BbsI9wHB6xEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from unsloth.chat_templates import get_chat_template, train_on_responses_only\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from datasets import Dataset\n",
        "import json"
      ],
      "metadata": {
        "id": "oMdDOVeJ6zXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load model"
      ],
      "metadata": {
        "id": "WeeE9yn_3_pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
        "max_seq_length = 2048\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_path,\n",
        "    max_seq_length = max_seq_length, # maximum sequence length for the model√ß\n",
        "    load_in_4bit = True, # use 4-bit quantization for memory efficiency\n",
        "    dtype = None,\n",
        ")\n",
        "\n",
        "# Apply the correct chat template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")"
      ],
      "metadata": {
        "id": "QQ1j-vfb4Bp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning with QLoRA"
      ],
      "metadata": {
        "id": "GUtDMV2W3d8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_path = \"train_data.jsonl\"\n",
        "output_dir = \"outputs\"\n",
        "num_epochs = 2\n",
        "lora_r = 16\n",
        "max_seq_length = 2048\n",
        "\n",
        "# Extract the data from the test_data file\n",
        "with open(train_data_path, 'r') as f:\n",
        "    train_data = [json.loads(line.strip()) for line in f]\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template([convo], tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    texts[1] = texts[1][len(\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>\"):]\n",
        "    return texts[0]+texts[1]\n",
        "\n",
        "def gen():\n",
        "    for code in train_data:\n",
        "        prompt = f\"\"\"Analyze the time complexity of the following code.\n",
        "    Choose exactly one of the following options: O(1), O(logn), O(n), O(nlogn), O(n^2), O(n^3) or exponential (O(2^n), O(3^n), etc.).\n",
        "    Give the time complexity of the code:\n",
        "    {code['src']}\"\"\"\n",
        "        yield {\"conversations\": [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": code['complexity']}],\n",
        "            \"text\": formatting_prompts_func({\"conversations\": [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": code['complexity']}]})}\n",
        "\n",
        "dataset = Dataset.from_list(list(gen()))\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_r,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = num_epochs,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = output_dir,\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "e6wxde-0o3Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "EAT63yXlpGLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_path = \"test_data.jsonl\"\n",
        "save_path = \"outputs/test_results.jsonl\"\n",
        "max_new_tokens = 1024\n",
        "\n",
        "# Extract the data from the test_data file\n",
        "with open(test_data_path, 'r') as f:\n",
        "    test_data = [json.loads(line.strip()) for line in f]\n",
        "\n",
        "with open(save_path, 'w') as f:\n",
        "    for code in test_data:\n",
        "        # Change the prompt to match the training data format\n",
        "        prompt = f\"\"\"Analyze the time complexity of the following code.\n",
        "    Choose exactly one of the following options: O(1), O(logn), O(n), O(nlogn), O(n^2), O(n^3) or exponential (O(2^n), O(3^n), etc.).\n",
        "    Give the time complexity of the code:\n",
        "    {code['src']}\"\"\"\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        if len(inputs[0]) > max_seq_length:\n",
        "            continue\n",
        "\n",
        "        tokens = model.generate(input_ids=inputs,\n",
        "                                do_sample=False, # deterministic generation\n",
        "                                max_new_tokens=max_new_tokens, # maximum number of tokens to generate\n",
        "                                use_cache=True, # use cache for faster generation\n",
        "                                no_repeat_ngram_size=4) # to avoid repetition\n",
        "\n",
        "        result = tokenizer.decode(tokens[0],skip_special_tokens=True)\n",
        "        idx = result.find(\"assistant\")\n",
        "        result = result[idx + len(\"assistant\"):].lstrip() # Filter the output to get the assistant's response\n",
        "\n",
        "        entry = {\n",
        "            \"src\": code['src'],\n",
        "            \"complexity\": code['complexity'],\n",
        "            \"model\": result,\n",
        "        }\n",
        "        f.write(json.dumps(entry) + '\\n')"
      ],
      "metadata": {
        "id": "Dkx7cA8votdA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}